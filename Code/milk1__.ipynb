{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b89958-2536-4b38-a20b-be3cc7094993",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_state:  32\n",
      "type\n",
      "0    21\n",
      "1    20\n",
      "2    17\n",
      "3    22\n",
      "Name: count, dtype: int64\n",
      "type\n",
      "0    5\n",
      "1    5\n",
      "2    4\n",
      "3    6\n",
      "Name: count, dtype: int64\n",
      "Tree>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_tree\n",
    "import graphviz\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import re\n",
    "\n",
    "\n",
    "save_the_Data=False\n",
    "\n",
    "Treatment=pd.read_csv(\"../Ai code/milk1.csv\")\n",
    "\n",
    "df_x=Treatment.iloc[:,1:]\n",
    "df_y=Treatment.iloc[:,0]\n",
    "R_state=random.randrange(1,100)\n",
    "print(\"Random_state: \",R_state)\n",
    "R_state=11\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=R_state,stratify=df_y)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.13,stratify=df_y)\n",
    "\n",
    "\n",
    "print(y_train.value_counts().sort_index())\n",
    "#print(X_test)\n",
    "#print(y_train.value_counts().sort_index())\n",
    "#print(y_train)\n",
    "#print(X_train)\n",
    "print(y_test.value_counts().sort_index())\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Tree>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "\n",
    "\n",
    "clf=XGBClassifier(tree_method=\"hist\",\n",
    "               # colsample_bynode=0.3,\n",
    "                  #  colsample_bylevel=0.2,\n",
    "                 # colsample_bytree=0.8,\n",
    "                    subsample=0.6,\n",
    "                   # min_child_weight=0.2,\n",
    "                    objective=\"reg:pseudohubererror\",\n",
    "                   # max_bin=2056,\n",
    "                     eta=0.04,\n",
    "                  reg_alpha=0.1,\n",
    "                  #num_parallel_tree=15,\n",
    "                  reg_lambda=0.07,\n",
    "                  n_estimators=670,\n",
    "                  max_depth=None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf=XGBClassifier(tree_method=\"approx\",\n",
    "                            eta=0.05,\n",
    "                  max_depth=20,\n",
    "                             reg_lambda=0.7,\n",
    "                            reg_alpha=0.7,\n",
    "                           # subsample=0.8,\n",
    "                  max_bin=5012,\n",
    "                            colsample_bynode=0.7,\n",
    "                            colsample_bylevel=0.8,\n",
    "                  colsample_bytree=0.6,\n",
    "                 objective=\"reg:pseudohubererror\",\n",
    "                  #num_parallel_tree=15,\n",
    "                 )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "    \n",
    "    \n",
    "def testing_value():\n",
    "    scores =[]\n",
    "    a=1\n",
    "    b=100\n",
    "    c=5\n",
    "    high_score=[0,0]\n",
    "    for k in range(a,b,c):\n",
    "        print(\"On: \",k)\n",
    "        R_state=k\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=R_state,stratify=df_y)\n",
    "        \n",
    "        rfc=XGBClassifier(tree_method=\"approx\",\n",
    "                            eta=0.05,\n",
    "                  max_depth=20,\n",
    "                             reg_lambda=0.7,\n",
    "                            reg_alpha=0.7,\n",
    "                           # subsample=0.8,\n",
    "                  max_bin=5012,\n",
    "                            colsample_bynode=0.7,\n",
    "                            colsample_bylevel=0.8,\n",
    "                  colsample_bytree=0.6,\n",
    "                 objective=\"reg:pseudohubererror\",\n",
    "                 # num_parallel_tree=15,\n",
    "                 )\n",
    "\n",
    "        rfc.fit(X_train, y_train)\n",
    "        y_pred = rfc.predict(X_test)\n",
    "        y_pred = (y_pred > 0.5)\n",
    "        if (accuracy_score(y_test, y_pred)*100)>high_score[0]:\n",
    "            high_score=[(accuracy_score(y_test, y_pred)*100),(k*100)]\n",
    "            print(\"hi \",k)\n",
    "        print(\"Accuracy:\", (accuracy_score(y_test, y_pred)*100),\"%\")\n",
    "        scores.append(accuracy_score(y_test, y_pred))\n",
    "        #print(\"<><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\")\n",
    "    print(\"High score: \",high_score[1])\n",
    "    %matplotlib inline    \n",
    "    plt.plot(range(a,b,c), scores)\n",
    "    plt.xlabel('Value of n_estimators for Random Forest Classifier')\n",
    "    plt.ylabel('Testing Accuracy')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def accuracy_report():\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred = (y_pred > 0.5) \n",
    "    #print(\"Y_pred\")\n",
    "    #print(y_pred)\n",
    "    #print(\"Y_test\")\n",
    "    #print(y_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", (accuracy*100),\"%\")\n",
    "    print(classification_report(y_test,y_pred,output_dict=False))\n",
    "    classificationReport = classification_report(y_test, y_pred)\n",
    "\n",
    "    plot_classification_report(classificationReport)\n",
    "\n",
    "\n",
    "def plot_classification_report(cr, title='Classification report ', with_avg_total=False, cmap=plt.cm.hot):\n",
    "\n",
    "    lines = cr.split('\\n')\n",
    "\n",
    "    classes = []\n",
    "    plotMat = []\n",
    "    for line in lines[2 : (len(lines) - 6)]:\n",
    "        t = line.split()\n",
    "        classes.append(t[0])\n",
    "        v = [float(x) for x in t[1: len(t) - 1]]\n",
    "        plotMat.append(v)\n",
    "\n",
    "    if with_avg_total:\n",
    "        aveTotal = lines[len(lines) - 1].split()\n",
    "        classes.append('avg/total')\n",
    "        vAveTotal = [float(x) for x in t[1:len(aveTotal) - 1]]\n",
    "        plotMat.append(vAveTotal)\n",
    "\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.imshow(plotMat, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    x_tick_marks = np.arange(3)\n",
    "    y_tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(x_tick_marks, ['precision', 'recall', 'f1-score'], rotation=45)\n",
    "    #classes=[\"semi-skimmed\",\"Whole\",\"Skimmed\",\"Alternative\",\"poultry\",\"Pork\",\"Beef\",\"Lamb\",\"apple\",\"orange\",\"tomatoes\",\"bananas\",\"grape\",\"Carrots\",\"Potato\",\"Onions\",\"Mushroom\"]\n",
    "    #plt.yticks(y_tick_marks,classes)\n",
    "    plt.yticks(y_tick_marks)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Classes')\n",
    "    plt.xlabel('Measures')\n",
    "\n",
    "\n",
    "    \n",
    "def plot_the_tree():\n",
    "    fig, ax = plt.subplots(figsize=(120,20))\n",
    "    tree.plot_tree(clf,ax=ax, proportion=True, filled=True, fontsize=20)\n",
    "    #tree.plot_tree(clf,ax=ax, proportion=True, filled=True, fontsize=20)\n",
    "    def replace_text(obj):\n",
    "        if type(obj) == matplotlib.text.Annotation:\n",
    "            txt = obj.get_text()\n",
    "            txt=re.split(\"%\",txt,1)\n",
    "            txt = re.sub(\"\\nsamples[^$]*class\",\"\\nclass\",txt[0])\n",
    "            obj.set_text(txt)\n",
    "        return obj    \n",
    "    ax.properties()['children'] = [replace_text(i) for i in ax.properties()['children']]\n",
    "    plt.savefig('DTC keras')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def plot_xgb_tree():\n",
    "    plt.figure(figsize=(400, 120))\n",
    "    plot_tree(clf)\n",
    "    plt.savefig('DTC keras', dpi=1200)\n",
    "    \n",
    "    \n",
    "    f = open(\"out.txt\", \"w\")\n",
    "    var=clf.get_booster().get_dump()\n",
    "    for line in var:\n",
    "        f.write(f\"{line}\\n\")\n",
    "    f.close()\n",
    "    \n",
    "#testing_value()\n",
    "accuracy_report()\n",
    "plot_xgb_tree()\n",
    "#plot_the_tree()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3fa609-d6e9-4bb7-8400-990d03bbf25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
